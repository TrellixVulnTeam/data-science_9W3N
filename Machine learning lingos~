- read NLP
- read tensorflow + work on the problem => video?

- listen to videos


1. Bias vs Variance :
   Bias - error in training set 
   Variance - error in test set
   - Tend to trade off each other Bias vs Variance. 
2. Probability vs likelihood. 
  - let say  we have a distribution of the weight of the population. let's assume the distribution is normal. Then the prob of person weighing between 180-200 lbs is the area (the width) between these 2 points. While the likelihood of someone weighing 200 lbs is the point(height) on the curve. So the probability could 30% while likelihood could be 0.15 

3. continous vs discrete(can be numeric[number of apples] and categrical[apple, orange])

4. dimensionality reduction: 
   A) PCA: Company MCap, # of employees, Revenue, ..
                         - minimal SS (Distances)= Eigenvalue => fitted line 1 = PC1
                         - sq root of Eigenvalue = Singular Value for PC1
                         - PS2 = perpendicular(orthogonal) line to PC1
          - Goal: Reducing the number of variables(simplicity) of a data at the expense of accuracy.
          1) Standadize it: z = (value-mean)/std => Otherwise larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. So, transforming the data to comparable scales can prevent this problem.
          2)  covariance matrix - To identify these correlations
               or use SVD
          3)  Compute the eigenvectors and eigenvalues of the covariance matrix to identify the PC's
           - minimal SS (Distances)= or max Variance = Eigenvalue => fitted line 1 = PC1 => then create a orthogonal(perpendicular) PC2 
           => ...
           - Create principal components (max  number of variables or number of samples whichever is less.)
           - first PC1 contacts the most info: Highest variance, then second and so on! 
           - Eigenvectors Covariance matrix = directions of the axes where there is the most variance (most information)
           - Eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each PC
           - Eigenvalues = what % of total info is contained by the PC = how important Eiginvectors are
           - Covariance = how much 2 variables change together
   B) Heatmap
   D) T-Sne
   D) Multi-Dimensional scaling

- linear discriminant analysis

