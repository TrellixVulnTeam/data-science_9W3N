data cleaning: - 

Preprocessing: 
    

0. Make sure numeric and objects are appropriate columns.
1. outliers: apply interquartile range or z score and transform the outliers to min, max z score or interquartile range.
2. make sure no data is missing: check => data imputation. 
                                       => researches suggest over and over again FuzzyKMeans() seem produce the best results. 
    data imputation: a) Categorical: mode
                     b) Numeric:
                        continuous data interpolation:  populate missing values using representative values from similar rows. => common for time 
                                                                                                                           series data

3. skewnewss (numeric data): box - cox transformation or apply log => from scipy.special import boxcox1p vs sklearn powertransformer()
4. scale the data => numeric vs categorical
5. encode it.   
    a)  Categorical - onehotencoding (pd.dummies but numpy works better with bigger dataset) = multi label
                     labelencoder(pd.) = binary label = factorize => could label nan's as 0! But don't do that!
                     labelbinarizer(seem to perform the best and a better version of onehot with lower dimensions) => But it uses more 
                                                                                      processing power and might throw some memory errors
                              from sklearn.preprocessing import LabelBinarizer 
                              lb = LabelBinarizer()
                              lb_res = lb.fit_transform(df["cat_col"])
                              pd.DataFrame(lb_res, columns=lb.classes_).head()
                              (first the categories are encoded as ordinal, then those integers are converted into binary code, then the digits from that binary string are split into separate columns.  This encodes the data in fewer dimensions that one-hot, but with some distortion of the distances.) => best result
                     Backward Difference: the mean of the dependent variable for a level is compared with the mean of the dependent variable for the prior level. This type of coding may be useful for a nominal or an ordinal variable.
                     Polynomial: The coefficients taken on by polynomial coding for k=4 levels are the linear, quadratic, and cubic trends in the categorical variable. The categorical variable here is assumed to be represented by an underlying, equally spaced numeric variable. Therefore, this type of encoding is used only for ordered categorical variables with equal spacing.
                     ref: https://pbpython.com/categorical-encoding.html
                          https://www.kdnuggets.com/2015/12/beyond-one-hot-exploration-categorical-variables.html
     b) numeric- 
6. Balance the data => for fraud detection, rare disease detection, anomaly detection
                    => Use synthetic sampling methods like SMOTE and MSMOTE along with advanced boosting methods like Gradient boosting and XG Boost.
                    from imblearn.over_sampling import SMOTENC
                    smote_nc = SMOTENC(categorical_features=[0, 2], random_state=0)
                    X_resampled, y_resampled = smote_nc.fit_resample(X, y)
                   Interpolation
Oversamlping: Smote
Undersampling: ....

7. Mean encode the data

8. nested cross val => to fight overfitting

spam => knn classifier
     => logistic regression
     => random forest

amazon reviews => naive bayesian

Data leakage:
Data preparation and feature engineering steps are done separately for the training and testing sets.

