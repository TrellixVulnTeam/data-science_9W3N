- For Perceptron models, the cost we are trying to minimize is the sum of the output of our functional form from all misclasified examples. 

- x1, x2, x3 .. w1, w2, w3 .. => -  binarize the output using threshhold:sigmoid function
 y - F(x1, x2, x3 .. w1, w2, w3 ) = error minimized => logistic function 0, 1

neural net = ensembe of perceptrons



activation function: 
1) sigmoid: 0, 1  => forces you choose between 0, 1 => 1 output
   softmax => 0.53
2) hyperbolic tangent: -1, 1: y=tahn(N)
3) RELU: rectified linear units: y = Sigma(wN) => if y < 0 then o, else y = y => it gives you the number that you out into activation function!




layers: visible payers + hidden layers => deep learning


Types: 

A) 1. Feedforward Neural Network â€“ Artificial Neuron: computer vision and speech recognition where classifying the target classes are complicated

B) radial basis: uses radial basis: power restoration

C) RNN: text to speech: similar to time series: feedforward

today =>  is => a good => day

D) CNN: image classification, signal processing => matrix multiplication, convolutions(linear equations)
 grid-like topology, such as an image.


Reinforcement learning is an incredibly useful and exciting form of data science, where a model gets feedback from its environment or system. The model tries potential actions at random, and then sees what kind of feedback it receives. In the end it learns a way to operate through a series of events that generates the most positive feedback.
