{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll work with two different NLP packages: NLTK and spaCy. NLP is good for learning language parsing because it is highly customizeable and transparent. On the other hand, it also contains many older models and methods that are useful for teaching NLP but are not optimal for production code. spaCy is almost the direct opposite. Rather than offering language parsing options, spaCy just processes text data using whatever algorithms and methods are considered \"state of the art\". It is considerably leaner, and because it is written in Cython (meaning Python code is translated into C and then run), it is considerably faster. On the other hand, we lose the virtue of choice, and if spaCy's algorithms change, our results could change as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "# Launch the installer to download \"gutenberg\" and \"stop words\" corpora.\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'austen-emma.txt', u'austen-persuasion.txt', u'austen-sense.txt', u'bible-kjv.txt', u'blake-poems.txt', u'bryant-stories.txt', u'burgess-busterbrown.txt', u'carroll-alice.txt', u'chesterton-ball.txt', u'chesterton-brown.txt', u'chesterton-thursday.txt', u'edgeworth-parents.txt', u'melville-moby_dick.txt', u'milton-paradise.txt', u'shakespeare-caesar.txt', u'shakespeare-hamlet.txt', u'shakespeare-macbeth.txt', u'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "# Import the data we just downloaded and installed.\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "\n",
    "# Grab and process the raw data.\n",
    "print(gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\nRaw:\\n', u\"[Alice's Adventures in Wonderland by Lewis Carroll 1865]\\n\\nCHAPTER I. Down the Rabbit-Hole\\n\\nAlice was\")\n"
     ]
    }
   ],
   "source": [
    "persuasion = gutenberg.raw('austen-persuasion.txt')\n",
    "alice = gutenberg.raw('carroll-alice.txt')\n",
    "\n",
    "# Print the first 100 characters of Alice in Wonderland.\n",
    "print('\\nRaw:\\n', alice[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Title removed:\\n', u'\\n\\nCHAPTER I. Down the Rabbit-Hole\\n\\nAlice was beginning to get very tired of sitting by her sister on')\n"
     ]
    }
   ],
   "source": [
    "#result = re.sub('abc',  '',    input)           # Delete pattern abc\n",
    "#result = re.sub('abc',  'def', input)           # Replace pattern abc -> def\n",
    "\n",
    "# This pattern matches all text between square bracket and removes it!.\n",
    "pattern = \"[\\[].*?[\\]]\"\n",
    "#pattern2 = \"\\n\"\n",
    "\n",
    "persuasion = re.sub(pattern, \"\", persuasion)\n",
    "alice = re.sub(pattern, \"\", alice)\n",
    "#alice = re.sub(pattern2, \"\", alice)\n",
    "\n",
    "\n",
    "# Print the first 100 characters of Alice again.\n",
    "print('Title removed:\\n', alice[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter headings removed:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Alice was beginning to get very tired of sitting by her sister on the\n",
      "bank, and of having nothin\n"
     ]
    }
   ],
   "source": [
    "# Now we'll match and remove chapter headings.\n",
    "persuasion = re.sub(r'Chapter \\d+', '', persuasion)\n",
    "alice = re.sub(r'CHAPTER .*', '', alice)\n",
    "\n",
    "# Ok, what's it look like now?\n",
    "print'Chapter headings removed:\\n', alice[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra whitespace removed:\n",
      "Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to\n"
     ]
    }
   ],
   "source": [
    "# Remove newlines and other extra whitespace by splitting and rejoining.\n",
    "persuasion = ' '.join(persuasion.split())\n",
    "alice = ' '.join(alice.split())\n",
    "\n",
    "# All done with cleanup? Let's see how it looks.\n",
    "print'Extra whitespace removed:\\n', alice[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What information can we extract from text?\n",
    "\n",
    "Tokens\n",
    "\n",
    "Each individual meaningful piece from a text is called a token, and the process of breaking up the text into these pieces is called tokenization. Tokens are generally words and punctuation. We may discard some tokens, such as punctuation, that we don't think add informational value. One class of potentially-uninformative tokens is stop words, words used very frequently that don't have much informational value, such as \"the\" and \"of\". Some NLP approaches discard stop words, while other approaches retain them because stop words can make up part of meaningful phrases (\"master of the universe\" being more specific and informative than \"master\" and \"universe\" alone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u\"you're\", u\"you've\", u\"you'll\", u\"you'd\", u'your', u'yours', u'yourself', u'yourselves', u'he', u'him', u'his', u'himself', u'she', u\"she's\", u'her', u'hers', u'herself', u'it', u\"it's\", u'its', u'itself', u'they', u'them', u'their', u'theirs', u'themselves', u'what', u'which', u'who', u'whom', u'this', u'that', u\"that'll\", u'these', u'those', u'am', u'is', u'are', u'was', u'were', u'be', u'been', u'being', u'have', u'has', u'had', u'having', u'do', u'does', u'did', u'doing', u'a', u'an', u'the', u'and', u'but', u'if', u'or', u'because', u'as', u'until', u'while', u'of', u'at', u'by', u'for', u'with', u'about', u'against', u'between', u'into', u'through', u'during', u'before', u'after', u'above', u'below', u'to', u'from', u'up', u'down', u'in', u'out', u'on', u'off', u'over', u'under', u'again', u'further', u'then', u'once', u'here', u'there', u'when', u'where', u'why', u'how', u'all', u'any', u'both', u'each', u'few', u'more', u'most', u'other', u'some', u'such', u'no', u'nor', u'not', u'only', u'own', u'same', u'so', u'than', u'too', u'very', u's', u't', u'can', u'will', u'just', u'don', u\"don't\", u'should', u\"should've\", u'now', u'd', u'll', u'm', u'o', u're', u've', u'y', u'ain', u'aren', u\"aren't\", u'couldn', u\"couldn't\", u'didn', u\"didn't\", u'doesn', u\"doesn't\", u'hadn', u\"hadn't\", u'hasn', u\"hasn't\", u'haven', u\"haven't\", u'isn', u\"isn't\", u'ma', u'mightn', u\"mightn't\", u'mustn', u\"mustn't\", u'needn', u\"needn't\", u'shan', u\"shan't\", u'shouldn', u\"shouldn't\", u'wasn', u\"wasn't\", u'weren', u\"weren't\", u'won', u\"won't\", u'wouldn', u\"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# Here is a list of the stopwords identified by NLTK.\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and use spaCy to parse our novels into tokens. When we call spaCy on the novel it will immediately and automatically parse it, tokenizing the string by breaking it into words and punctuation (and many other things we will explore)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "# All the processing work is done here, so it may take a while.\n",
    "alice_doc = nlp(alice)\n",
    "persuasion_doc = nlp(persuasion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The alice_doc object is a <type 'spacy.tokens.doc.Doc'> object.\n",
      "It is 34408 tokens long\n",
      "The first three tokens are 'Alice was beginning'\n",
      "The type of each token is <type 'spacy.tokens.token.Token'>\n"
     ]
    }
   ],
   "source": [
    "# Let's explore the objects we've built.\n",
    "print(\"The alice_doc object is a {} object.\".format(type(alice_doc)))\n",
    "print(\"It is {} tokens long\".format(len(alice_doc)))\n",
    "print(\"The first three tokens are '{}'\".format(alice_doc[:3]))\n",
    "print(\"The type of each token is {}\".format(type(alice_doc[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Alice:', [(u'the', 1524), (u'and', 796), (u'to', 724), (u'a', 611), (u'I', 533), (u'it', 524), (u'she', 508), (u'of', 499), (u'said', 453), (u'Alice', 394)])\n",
      "('Persuasion:', [(u'the', 3120), (u'to', 2775), (u'and', 2738), (u'of', 2563), (u'a', 1529), (u'in', 1346), (u'was', 1329), (u'had', 1177), (u'her', 1159), (u'I', 1118)])\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Utility function to calculate how frequently words appear in the text.\n",
    "def word_frequencies(text, include_stop=True):\n",
    "    \n",
    "    # Build a list of words.\n",
    "    # Strip out punctuation and, optionally, stop words.\n",
    "    words = []\n",
    "    for token in text:\n",
    "        if not token.is_punct and (not token.is_stop or include_stop):\n",
    "            words.append(token.text)\n",
    "            \n",
    "    # Build and return a Counter object containing word counts.\n",
    "    return Counter(words)\n",
    "    \n",
    "# The most frequent words:\n",
    "alice_freq = word_frequencies(alice_doc).most_common(10)\n",
    "persuasion_freq = word_frequencies(persuasion_doc).most_common(10)\n",
    "print('Alice:', alice_freq)\n",
    "print('Persuasion:', persuasion_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Alice:', [(u'said', 453), (u'Alice', 394), (u'little', 124), (u'like', 84), (u'went', 83), (u'know', 83), (u'thought', 74), (u'Queen', 73), (u'time', 68), (u'King', 61)])\n",
      "('Persuasion:', [(u'Anne', 496), (u'Captain', 297), (u'Mrs', 291), (u'Elliot', 288), (u'Mr', 254), (u'Wentworth', 217), (u'Lady', 191), (u'good', 181), (u'little', 175), (u'Charles', 166)])\n"
     ]
    }
   ],
   "source": [
    "# Use our optional keyword argument to remove stop words.\n",
    "alice_freq = word_frequencies(alice_doc, include_stop=False).most_common(10)\n",
    "persuasion_freq = word_frequencies(persuasion_doc, include_stop=False).most_common(10)\n",
    "print('Alice:', alice_freq)\n",
    "print('Persuasion:', persuasion_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Popular words that are not common in both books to see the characteristics of each book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Unique to Alice:', set([u'King', u'said', u'like', u'Queen', u'Alice', u'thought', u'know', u'time', u'went']))\n",
      "('Unique to Persuasion:', set([u'good', u'Elliot', u'Charles', u'Mrs', u'Mr', u'Anne', u'Captain', u'Lady', u'Wentworth']))\n"
     ]
    }
   ],
   "source": [
    "# Pull out just the text from our frequency lists.\n",
    "alice_common = [pair[0] for pair in alice_freq]\n",
    "persuasion_common = [pair[0] for pair in persuasion_freq]\n",
    "\n",
    "# Use sets to find the unique values in each top ten.\n",
    "print('Unique to Alice:', set(alice_common) - set(persuasion_common))\n",
    "print('Unique to Persuasion:', set(persuasion_common) - set(alice_common))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmas\n",
    "\n",
    "Words \"think\", \"thought\", and \"thinking\" have a common root in \"think\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmas: \n",
      "\n",
      "\n",
      "Alice: [(u'say', 477), (u'Alice', 394), (u'think', 130), (u'go', 130), (u'little', 125), (u'look', 106), (u'know', 103), (u'come', 96), (u'like', 92), (u'begin', 91)]\n",
      "Persuasion: [(u'Anne', 493), (u'Captain', 294), (u'Mrs', 291), (u'Elliot', 288), (u'think', 256), (u'know', 255), (u'Mr', 254), (u'good', 224), (u'Wentworth', 215), (u'say', 191)]\n",
      "Unique to Alice: set([u'begin', u'look', u'little', u'Alice', u'go', u'come', u'like'])\n",
      "Unique to Persuasion: set([u'good', u'Elliot', u'Mrs', u'Mr', u'Anne', u'Captain', u'Wentworth'])\n",
      "\n",
      "\n",
      "Prefix: \n",
      "\n",
      "\n",
      "Alice: [(u's', 1378), (u't', 842), (u'l', 655), (u'c', 630), (u'w', 498), (u'h', 453), (u'f', 437), (u'A', 417), (u'g', 407), (u'b', 405)]\n",
      "Persuasion: [(u's', 3139), (u'c', 2273), (u'p', 1857), (u'a', 1784), (u'f', 1555), (u'h', 1420), (u'l', 1403), (u't', 1391), (u'd', 1384), (u'r', 1348)]\n",
      "Unique to Alice: set([u'A', u'b', u'w', u'g'])\n",
      "Unique to Persuasion: set([u'a', u'p', u'r', u'd'])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Utility function to calculate how frequently lemas appear in the text.\n",
    "def lemma_frequencies(text, include_stop=True, token='Lemma'):\n",
    "    \n",
    "    # Build a list of lemas.\n",
    "    # Strip out punctuation and, optionally, stop words.\n",
    "    lemmas = []\n",
    "    prefixes = [] #(token.prefix_) \n",
    "    suffixes =[] #(token.suffix_)\n",
    "    if token == 'Lemma': \n",
    "        for token in text:\n",
    "            if not token.is_punct and (not token.is_stop or include_stop):\n",
    "                lemmas.append(token.lemma_)             \n",
    "    elif token =='prefix':\n",
    "        for token in text:\n",
    "            if not token.is_punct and (not token.is_stop or include_stop):\n",
    "                lemmas.append(token.prefix_)  \n",
    "    elif token =='suffix':\n",
    "        for token in text:\n",
    "            if not token.is_punct and (not token.is_stop or include_stop):\n",
    "                lemmas.append(token.suffix_)              \n",
    "    # Build and return a Counter object containing word counts.\n",
    "    return Counter(lemmas)\n",
    "\n",
    "# Instantiate our list of most common lemmas.\n",
    "alice_lemma_freq = lemma_frequencies(alice_doc, include_stop=False).most_common(10)\n",
    "persuasion_lemma_freq = lemma_frequencies(persuasion_doc, include_stop=False).most_common(10)\n",
    "print 'Lemmas: \\n'\n",
    "\n",
    "\n",
    "print'\\nAlice:', alice_lemma_freq\n",
    "print'Persuasion:', persuasion_lemma_freq\n",
    "\n",
    "# Again, identify the lemmas common to one text but not the other.\n",
    "alice_lemma_common = [pair[0] for pair in alice_lemma_freq]\n",
    "persuasion_lemma_common = [pair[0] for pair in persuasion_lemma_freq]\n",
    "print 'Unique to Alice:', set(alice_lemma_common) - set(persuasion_lemma_common)\n",
    "print 'Unique to Persuasion:', set(persuasion_lemma_common) - set(alice_lemma_common)\n",
    "print '\\n'\n",
    "print 'Prefix:'\n",
    "\n",
    "alice_lemma_freq = lemma_frequencies(alice_doc, include_stop=False, token = 'prefix').most_common(10)\n",
    "persuasion_lemma_freq = lemma_frequencies(persuasion_doc, include_stop=False, token = 'prefix').most_common(10)\n",
    "print 'Prefix: \\n'\n",
    "\n",
    "\n",
    "print'\\nAlice:', alice_lemma_freq\n",
    "print'Persuasion:', persuasion_lemma_freq\n",
    "\n",
    "# Again, identify the lemmas common to one text but not the other.\n",
    "alice_lemma_common = [pair[0] for pair in alice_lemma_freq]\n",
    "persuasion_lemma_common = [pair[0] for pair in persuasion_lemma_freq]\n",
    "print 'Unique to Alice:', set(alice_lemma_common) - set(persuasion_lemma_common)\n",
    "print 'Unique to Persuasion:', set(persuasion_lemma_common) - set(alice_lemma_common)\n",
    "print '\\n'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentences: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice in Wonderland has 1727 sentences.\n",
      "Here is an example: \n",
      "There was nothing so VERY remarkable in that; nor did Alice think it so VERY much out of the way to hear the Rabbit say to itself, '\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initial exploration of sentences.\n",
    "sentences = list(alice_doc.sents)\n",
    "print(\"Alice in Wonderland has {} sentences.\".format(len(sentences)))\n",
    "\n",
    "example_sentence = sentences[2]\n",
    "print(\"Here is an example: \\n{}\\n\".format(example_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 27 words in this sentence, and 23 of them are unique.\n"
     ]
    }
   ],
   "source": [
    "# Look at some metrics around this sentence.\n",
    "example_words = [token for token in example_sentence if not token.is_punct]\n",
    "unique_words = set([token.text for token in example_words])\n",
    "\n",
    "print((\"There are {} words in this sentence, and {} of them are\"\n",
    "       \" unique.\").format(len(example_words), len(unique_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parts of speech, dependencies, entities\n",
    "Tokens within each sentence are also coded with the parts of speech they play. This is useful for distinguishing between _homographs_, words with the same spelling but different meaning (the umbrella term for this kind of linguistic feature is _polysemy_).  For example, the word \"break\" is a noun in \"I need a break\" but a verb in \"I need to break the glass\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOUN\n",
      "VERB\n"
     ]
    }
   ],
   "source": [
    "print nlp(u\"I need a break\")[3].pos_\n",
    "print nlp(u\"I need to break the glass\")[3].pos_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parts of speech:\n",
      "(u'There', u'ADV')\n",
      "(u'was', u'VERB')\n",
      "(u'nothing', u'NOUN')\n",
      "(u'so', u'ADV')\n",
      "(u'VERY', u'ADV')\n",
      "(u'remarkable', u'ADJ')\n",
      "(u'in', u'ADP')\n",
      "(u'that', u'DET')\n",
      "(u';', u'PUNCT')\n"
     ]
    }
   ],
   "source": [
    "# View the part of speech for some tokens in our sentence.\n",
    "print('\\nParts of speech:')\n",
    "for token in example_sentence[:9]:\n",
    "    print(token.orth_, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "https://nlp.stanford.edu/software/stanford-dependencies.shtml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dependencies:\n",
      "(u'There', u'expl', u'was')\n",
      "(u'was', u'ROOT', u'was')\n",
      "(u'nothing', u'attr', u'was')\n",
      "(u'so', u'advmod', u'VERY')\n",
      "(u'VERY', u'advmod', u'remarkable')\n",
      "(u'remarkable', u'amod', u'nothing')\n",
      "(u'in', u'prep', u'remarkable')\n",
      "(u'that', u'pobj', u'in')\n",
      "(u';', u'punct', u'was')\n"
     ]
    }
   ],
   "source": [
    "# View the dependencies for some tokens.\n",
    "print('\\nDependencies:')\n",
    "for token in example_sentence[:9]:\n",
    "    print(token.orth_, token.dep_, token.head.orth_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'PERSON', u'Alice')\n",
      "(u'DATE', u'the hot day')\n",
      "(u'PERSON', u'Alice')\n",
      "(u'PERSON', u'Rabbit')\n",
      "(u'PERSON', u'Rabbit')\n",
      "(u'PERSON', u'Alice')\n",
      "(u'PERSON', u'Alice')\n",
      "(u'PERSON', u'Alice')\n",
      "(u'ORDINAL', u'First')\n",
      "(u'CARDINAL', u'one')\n"
     ]
    }
   ],
   "source": [
    "#Extract the first ten entities with .etns method\n",
    "entities = list(alice_doc.ents)[0:10]\n",
    "for entity in entities:\n",
    "    print(entity.label_, ' '.join(t.orth_ for t in entity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set([u'Treacle', u'the Knave of Hearts', u'William the Conqueror.', u'Ou', u'William the Conqueror', u\"W. RABBIT'\", u'Brandy', u'Sing', u'a Lobster Quadrille', u'Tortoise', u'Shy', u'Stretching', u'Alice', u'Longitude', u'Pat', u'Canary', u'Duchess', u'Lobster', u\"I'M\", u'Mouse', u'Seaography', u'Latin Grammar', u'Shakespeare', u'Latitude', u'Run', u'Cheshire Puss', u'William', u'Game', u'Rabbit', u'Tortoise--', u'Rule Forty-two', u'King', u'Dinah', u'Ma', u'Bill', u'Cheshire', u'Elsie', u'Knave', u'FOOT', u'Turtle', u'Lacie', u'Hare', u'Queen', u'Hush', u'The Knave of Hearts', u'Said', u\"the King: '\", u'Panther', u'Magpie', u'Ada', u\"Alice)--'and\", u'Soles', u'HER', u'a Cheshire Cat,', u'Behead', u'Curiouser', u'Dinn', u'Boots', u'Down', u'Duck', u'Off--', u'Lory', u'Twinkle', u'Edwin', u'Normans--', u'Tis', u'Footman', u'Mercia', u'Swim', u'Tillie', u'Begin', u'Beau', u'FATHER WILLIAM', u'Mary Ann', u'Jack', u'Off', u'VERY', u'ALICE', u'Gryphon', u'Soup', u'Lizard', u'Owl', u'Majesty', u'Fury', u'INSIDE', u'Mine', u'Soo', u'Mabel', u'Edgar Atheling', u'Kings'])\n"
     ]
    }
   ],
   "source": [
    "# All of the uniqe entities spaCy thinks are people.\n",
    "people = [entity.text for entity in list(alice_doc.ents) if entity.label_ == \"PERSON\"]\n",
    "print(set(people))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
