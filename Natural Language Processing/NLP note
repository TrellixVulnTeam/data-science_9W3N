Terms: 
 1. Corpus = Whole literature
 2. Document = texts, sentences, paragraphs, sometimes whole text files.

Libraries: 
- NLTK: NLP tasks: tokenization, lemmatization, stemming, parsing, POS tagging, etc., ..
- Spacy: NLTK's better version (intro 2015)=> can be used with neural network
- sklearn - preprocessing, machine learning
- gensim: -  topic and vector space modeling, document similarity.Tfidf vectorization, word2vec, document2vec, lsa, lda 
          - large unsupervised learning; supports deep learning
          - doc2vec: numerical representation of sentence/paragraphs/documents 
          - lsa, lda : topic modeling
- Pattern: web and dom crawler, apis(facebook, twitter)

Ref:
: Spacy- https://stackabuse.com/python-for-nlp-tokenization-stemming-and-lemmatization-with-spacy-library/

0. Preprocessing: 
   - tokenization: process of breaking a document down into words, punctuation marks, numeric digits, etc.
                 token : individual meaningful piece from a text that is generally words and punctuation.
   - lemmatization
   - stemming
   - parsing
   - POS tagging

   Example: 
      a) tokenization: 
      # spacy
      import spacy as sp
      sentence4 = sp(u"Hello, I am non-vegetarian, email me at abc-xyz@gmai.com")
      
      for word in sentence4:
          print(word.text): Hello
                            ,
                            I
                            am
                            non
                            -
                           vegetarian
                           ,
                           email
                           me
                           at
                           abc-xyz@gmai.com

     # nltk
     from nltk.tokenize import sent_tokenize, word_tokenize
     data = "all work and no play"
     print(word_tokenize(data))# ['All', 'work', 'and', 'no', 'play']
     print (sent_tokenize(sentences)) #['sent1', 'sent2']

     #####################
     Stopwords:   potentially-uninformative tokens (caution: sometimes it can be useful: President of the USA)
     example: print(stopwords.words('english'))
     #####################
     b) Detecting Entities: entity such as a company, place, building, currency, institution, etc.
     sentence5 = sp(u'Manchester United is looking to sign Harry Kane for $90 million')  

     for entity in sentence.ents:
        print(entity.text + ' - ' + entity.label_ + ' - ' + str(spacy.explain(entity.label_)))
     # Manchester United - ORG - Companies, agencies, institutions, etc.
     # Harry Kane - PERSON - People, including fictional
     # $90 million - MONEY - Monetary values, including unit
     
     c) Detecting Nouns
    for noun in sentence5.noun_chunks:
        print(noun.text)
    # Manchester United
    # Harry Kane

     d) Stemming and Lemmatization:
     -  both generate the root form of the inflected words. The difference is that stem might not be an actual word whereas, lemma is an 
    actual language word. Stemming - Only NLTK, Lemmatization- both Spacy and NLTK
    # compute, computer, computing, computed, etc. You may want to reduce the words to their root form for the sake of uniformity
   
    -  Porter Stemmer and Snowball stemmers( slightly improved version of Porter). 
    Example: 

    import nltk
    from nltk.stem.porter import *
    stemmer = PorterStemmer()
    tokens = ['compute', 'computer', 'computed', 'computing']
    
    for token in tokens:
        print(stemmer.stem(token)) # comput
    - Lemmas: converts words in the second or third forms to their first form variants

    #compute -> compute, computer-> computer, computed-> compute, computing-> computing

    sentence7 = sp(u'A letter has been written')
    for word in sentence7:
        print(word.text, '->', word.lemma_)

     A ===> a
     letter ===> letter
     has ===> have
     been ===> be
     written ===> write


    e) parts of speech tagging: Recognized if the word is verb, noun, etc, ...

        Example: google, can => can be both verb and a noun 

        1) NLTK
        2) Spacy: 
        sen = sp(u"I like to play football.")
        
        for word in sen:
           print(f'{word.text:{12}} {word.pos_:{10}} {word.tag_:{8}} {spacy.explain(word.tag_)}')# adding 12, 10, 8 spaces in between

        I            PRON       PRP      pronoun, personal
        like         VERB       VBP      verb, non-3rd person singular present
        to           PART       TO       infinitival to
        play         VERB       VB       verb, base form
        football     NOUN       NN       noun, singular or mass
        .            PUNCT      .        punctuation mark, sentence closer

        Visualizing:
 
        from spacy import displacy
        sen = sp(u"I like to play football. I hated it in my childhood though")
        displacy.render(sen, style='dep', jupyter=True, options={'distance': 85})



I.  Representing text in numeric form: 

1) Bag of words = simplest - Scikit

   Use it to build a baseline model or if your dataset is small and context is domain specific, BoW may work better than Word Embedding. Context is very domain specific which means that you cannot find corresponding Vector from pre-trained word embedding models (GloVe, fastText etc).

Example: Doc1 = "I like to play football", Doc2 = "It is a good game", Doc3 = "I prefer football over rugby"
         vocabulary  = [I, like, to, play, football, it, is, a, good, game, prefer, over, rugby]
         Convert Doc1 into vector features based on the frequency of each words: Doc1, the feature vector will look like this: [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]

    How do you transform text to numbers? 

    TF-IDF: a) CountVectorizer = Bag of Words = count the word occurance. keyword or important signal will occur again and again.
              count_vec = CountVectorizer()
              count_occurs = count_vec.fit_transform([doc])
              countvectorizer.get_feature_names() => prints out the actual words
            b) TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document) = Countvectorizer
                     = Term Frequency, which measures how frequently a term occurs in a document = all terms are considered equally important. => 
             - norm_count_vec = TfidfVectorizer(use_idf=False, norm='l2')  = Normalized CountVectorizer = Tf id Vectorizer -  extremely high frequency may dominate the result and causing model bias. Normalization can be apply to pipeline easily.
 
    
               problem: 1. In practice, certain terms, such as "is", "of", and "that", may appear a lot of times but have little importance.
                        2. Also some common words that describe 2 different topics, such word "engine" may dominate two separate topics of "car" and "boat". So there's a need to weight down these words that occur in different sentences and weigh up words that don't occur much in 2 separate documents or topics. 
 
               => Thus we need to weigh down the frequent terms while scale up the rare ones, by computing the following:


            c) TF + IDF(Inverse Document Frequency)
             - IDF(t) = log_e(Total number of documents / Number of documents with term t in it).=  The idea behind the TF-IDF approach is that the words that occur less in all the documents and more in individual document contribute more towards classification.


Example:

Consider a document containing 100 words wherein the word cat appears 3 times. The term frequency (i.e., tf) for cat is then (3 / 100) = 0.03. Now, assume we have 10 million documents and the word cat appears in one thousand of these. Then, the inverse document frequency (i.e., idf) is calculated as log(10,000,000 / 1,000) = 4. Thus, the Tf-idf weight is the product of these quantities: 0.03 * 4 = 0.12.

               
             - norm_count_vec = TfidfVectorizer(use_idf=True, norm='l2')
             
TF-IDF vectorizer - Term Frequency-Inverse Document Frequency  - low frequency contribute more weights to the model. Word importance will be increased if the number of occurrence within same document (i.e. training record). On the other hand, it will be decreased if it occurs in corpus or vocabulary (i.e. other training records).

               tfidf_vec = TfidfVectorizer()
             
          options: 
          ngram_range=(1, 2) => unigrams bigrams => {'hi ': 0, 'bye': 1, 'run away': 2}
                             unigram = ['I', 'He', 'She', ..]
                             Bigrams = ['The quick', 'quick red', ...]
                              Trigrams =  ['the quick red', 'quick red fox', 'red fox jumps' ..]
                            => can be used for text prediction, topic modeling, ...

          max_features = 2500, which means that it only uses the 2500 most frequently occurring words to create a bag of words feature vector.
          max_df = 0.8 => include words that occur in a maximum of 80% of the documents. Words that occur in all documents are too common and are not very useful for classification. Similarly, => 'Duh'
          min_df = 0.7 => include words that occur in at least 70% of the documents.


# ref: 
https://datascience.stackexchange.com/questions/19160/why-word2vec-performs-much-worst-than-both-countvectorizer-and-tfidfvectorizer
https://towardsdatascience.com/3-basic-approaches-in-bag-of-words-which-are-better-than-word-embeddings-c2cbc7398016


2) Word embedding: more advanced than bag-of-word (BoW), Latent Dirichlet Allocation (LDA), Latent Semantic Analysis (LSA) => these can lead to high dimensional vector, sparse feature, while Word embedding leads to dense features in low dimensions. 
  - Can use pre-trained model.
  - have lots and lots of text data in the relevant domain. For example, if your goal is to build a sentiment lexicon, then using a dataset from the medical domain or even wikipedia may not be effective.

  - Uses either Skipgram or Cbow
    Skipgram:  predicts the surrounding context words given current word. => worls better with large corpus 
    Cbow: predicts the current word based on the surrounding context.

  
   a) Gensim provide a amazing wrapper so that we can adopt different pre-trained word embedding models which includes Word2Vec (by Google), GloVe (by Stanford), fastText (by Facebook). => most_similiar, dissimilar, predict next word...
   
Fasttext: # Skipgram model :
          model = fasttext.train_unsupervised('data.txt', model='skipgram')
          # or, cbow model :
          model = fasttext.train_unsupervised('data.txt', model='cbow')

word_embedding:
   WordEmbedding().load(source='word2vec', file_path=word2vec_file_path)# source can word2vec, glove, fastText
 
   word2vec.Word2Vec.load_word2vec_format('vectors.bin') => uses pretrained model  
   b) Visualization: 
  - We can visualize it by using PCA or T-distributed Stochastic Neighbor Embedding (t-SNE)
  word_embedding.build_visual_metadata(embedding=embedding, words=words, file_dir='./word_embedding')

Maximum model size of GloVe, Word2Vec and fasttext are ~5.5GB, ~3.5GB and ~8.2GB respectively. It takes about 9, 1, 9 minutes for GloVe, Word2Vec and fasttext respectively. It may not easier to deploy to production with limited resource.

# ref: https://towardsdatascience.com/3-silver-bullets-of-word-embedding-in-nlp-10fa8f50cc5a

II. Topic modeling: Unsupervised NLP = applies clustering in NLP =  groups large volumes of unlabeled text data.
   => all of the below can be used to create features for other models, or try to gain insight from decompositions themselves.

- Topic modeling is an unsupervised technique that intends to analyze large volumes of text data by clustering the documents into groups. In the case of topic modeling, the text data do not have any labels attached to it. Rather, topic modeling tries to group the documents into clusters based on similar characteristics.A typical example of topic modeling is clustering a large number of newspaper articles that belong to the same category. In other words, cluster documents that have the same topic. It is important to mention here that it is extremely difficult to evaluate the performance of topic modeling since there are no right answers. It depends upon the user to find similar characteristics between the documents of one cluster and assign it an appropriate label or topic.
    
    Assumption: 
    - each document consists of a mixture of topics, and
    - each topic consists of a collection of words.
    A. LSA: (aka sklearn.decomposition.TruncatedSVD)
LSA (also known as Latent Semantic Analysis, LSA) learns latent topics by performing a matrix decomposition (TruncatedSVD or  PCA ) on the term-document matrix. LDA is a generative probabilistic model, that assumes a Dirichlet prior over the latent topics. In practice, LSI is much faster to train than LDA, but has lower accuracy. 

- Good at identifying synonyms but not polysemy(break a leg vs broken leg)

When we represent the words in the vector space, the cosine similarity (between 0 to 1)is used to find the similar words, topics, sentences as it is used in the search engine.  But vector representation of the text can be computationally expensive. That's where LSA comes in. 


Latent Semantic Analysis is the process of applying PCA to a tf-idf term-document matrix to get clusters of terms that presumably reflect a topic.  Each document will get a score for each topic, with higher scores indicating that the document is relevant to the topic. Documents can pertain to more than one topic.

LSA is used for a) corpus is too large
                b) when you don't know what topics characterize your documents. 
                c) To create features to be used in other models.

    B. LDA: (aka sklearn.decomposition.LatentDirichletAllocation)
        Assumptions: 
        1. Documents that have similar words usually have the same topic:  "economy", "profit", "the stock market", "loss"=> BUsiness Topic
        2. Documents that have groups of words frequently occurring together usually have the same topic:  if these words frequently occur together in multiple documents, those documents may belong to the same category.

    C. pLSA = probabilistic LSA => will yield similar results to LDA but faster => may overfit
    D. lda2Vec - Deep learning based
    E. NMF (aka decomposition.NMF) => much faster
 

   Example: 
   B. 
   LDA - The data set contains user reviews for different products in the food category. We will use LDA to group the user reviews into 5 categories.
   import pandas as pd
   import numpy as np
   from sklearn.decomposition import LatentDirichletAllocation  (diriklet)
   from sklearn.feature_extraction.text import CountVectorizer

   reviews_datasets = pd.read_csv(r'E:\Datasets\Reviews.csv')
   reviews_datasets = reviews_datasets.head(20000)
   reviews_datasets.dropna()
   # Before we can apply LDA, we need to create vocabulary of all the words in our data.
   count_vect = CountVectorizer(max_df=0.8, min_df=2, stop_words='english')
   doc_term_matrix = count_vect.fit_transform(reviews_datasets['Text'].values.astype('U'))
   # Include those words that appear in less than 80% of the document and appear in at least 2 documents and remove all the stop words

   LDA = LatentDirichletAllocation(n_components=5, random_state=42) # n_components specifies the number of categories, or topics
   LDA.fit(doc_term_matrix)
   # Let's find 10 words with the highest probability for the first topic. 
   first_topic = LDA.components_[0] # 0 = first topic
   top_topic_words = first_topic.argsort()[-10:] # 10 words => array([14106,  5892,  7088,  4290, 12596,  5771,  5187, 12888,  7498,
                                                 # 12921], dtype=int64)
 
   for i in top_topic_words:
    print(count_vect.get_feature_names()[i]) # water great just drink sugar good flavor taste like tea
   # Let's print the 10 words with highest probabilities for all the five topics:

   for i,topic in enumerate(LDA.components_):
      print(f'Top 10 words for topic #{i}:')
      print([count_vect.get_feature_names()[i] for i in topic.argsort()[-10:]])
      print('\n')

   Top 10 words for topic #0:
   ['water', 'great', 'just', 'drink', 'sugar', 'good', 'flavor', 'taste', 'like', 'tea']
 
   Top 10 words for topic #1:
   ['br', 'chips', 'love', 'flavor', 'chocolate', 'just', 'great', 'taste', 'good', 'like']

   Top 10 words for topic #2: 
   ['just', 'drink', 'orange', 'sugar', 'soda', 'water', 'like', 'juice', 'product', 'br']

   Top 10 words for topic #3:
   ['gluten', 'eat', 'free', 'product', 'like', 'dogs', 'treats', 'dog', 'br', 'food']

   Top 10 words for topic #4:
   ['cups', 'price', 'great', 'like', 'amazon', 'good', 'br', 'product', 'cup', 'coffee']

   # As a final step, we will add a column to the original data frame that will store the topic for the text. To do so, we can use    
   # LDA.transform() method and pass it our document-term matrix. This method will assign the probability of all the topics to each 
   # document.
   topic_values = LDA.transform(doc_term_matrix)
   topic_values.shape # (20000, 5) => each of the document has 5 columns where each column corresponds to the probability
   # create a new dataset
   reviews_datasets['Topic'] = topic_values.argmax(axis=1)

   E. NMF - Non-Negative Matrix Factorization- supervised learning technique which performs clustering as well as dimensionality reduction

   #-  It fixes values for the probability vectors of the multinomials, whereas LDA allows the topics and words themselves to vary.
   #Thus, in cases where we believe that the topic probabilities should remain fixed per document (oftentimes unlikely)—or in small data 
   #settings in which the additional variability coming from the hyperpriors is too much—NMF performs better.

   tfidf_vect = TfidfVectorizer(max_df=0.8, min_df=2, stop_words='english')
   doc_term_matrix = tfidf_vect.fit_transform(reviews_datasets['Text'].values.astype('U')
   from sklearn.decomposition import NMF

   nmf = NMF(n_components=5, random_state=42)
   nmf.fit(doc_term_matrix )

   first_topic = nmf.components_[0]
   top_topic_words = first_topic.argsort()[-10:]

   topic_values = nmf.transform(doc_term_matrix)
   reviews_datasets['Topic'] = topic_values.argmax(axis=1)
   reviews_datasets.head()

III. Other topics:


 Word2Vec(words) vs SensetoVec vs Sentence2Vec(sentences) vcs Doc2Vec (documents) => Gensim allows for all

    A. Word2Vec - neural network approach. => better than LSA => can identify polysemy
                - creates representations of individual words, based on the words around them. vs LSA (creates vector representations of sentences based on the words in them)
                - understand the meaning of words better based on the context
                - used for analogies ("king" is to "queen" as "man" is to "woman")
                - used for logical expressions ("king" + "woman" - "man" = ?) 

    B. Sense2vec = an upgrade to Word2vec => Word2vec + POS-(noun, verb, adjective, et)
                   "give me a break" vs "break a leg"
                    becomes: break_verb vs break_noun


https://kanoki.org/2019/03/07/sentence-similarity-in-python-using-doc2vec/


# reference: https://www.guru99.com/word-embedding-word2vec.html


IV. Text summarization: Extractive vs. abstractive

  or use TextRank algorithm that ranks texts based on similarity! it based on Google's PageRank algorithm. => if the text has a high rank, that means it has a lot of similarity with other texts, therefore can be used to summarize the entire corpus!
